ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 6: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 7: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
build: 4479 (a4f3f5d8) with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu
llama_model_load_from_file: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA1 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA2 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA3 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA4 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA5 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA6 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA7 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 254 tensors from mymodel/gemma-7b-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 7b
llama_model_loader: - kv   3:                       general.organization str              = Google
llama_model_loader: - kv   4:                           general.basename str              = gemma
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   7:                     gemma.embedding_length u32              = 3072
llama_model_loader: - kv   8:                          gemma.block_count u32              = 28
llama_model_loader: - kv   9:                  gemma.feed_forward_length u32              = 24576
llama_model_loader: - kv  10:                 gemma.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gemma.attention.head_count_kv u32              = 16
llama_model_loader: - kv  12:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  14:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                tokenizer.ggml.eot_token_id u32              = 107
llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - kv  29:                          general.file_type u32              = 7
llama_model_loader: - type  f32:   57 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 8.45 GiB (8.50 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 217
load: token to piece cache size = 1.6014 MB
print_info: arch             = gemma
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 256
print_info: n_swa            = 0
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 4096
print_info: n_embd_v_gqa     = 4096
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 24576
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 8.54 B
print_info: general.name     = Gemma 7b
print_info: vocab type       = SPM
print_info: n_vocab          = 256000
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 107 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 227 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 107 '<end_of_turn>'
print_info: max token length = 48
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =  1122.09 MiB
load_tensors:        CUDA1 model buffer size =  1122.09 MiB
load_tensors:        CUDA2 model buffer size =   841.57 MiB
load_tensors:        CUDA3 model buffer size =  1122.09 MiB
load_tensors:        CUDA4 model buffer size =  1122.09 MiB
load_tensors:        CUDA5 model buffer size =   841.57 MiB
load_tensors:        CUDA6 model buffer size =  1122.09 MiB
load_tensors:        CUDA7 model buffer size =  1357.93 MiB
load_tensors:   CPU_Mapped model buffer size =   796.88 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 512
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache_init:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache_init:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA7 KV buffer size =    64.00 MiB
llama_init_from_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     3.91 MiB
llama_init_from_model: pipeline parallelism enabled (n_copies=4)
llama_init_from_model:      CUDA0 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA1 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA2 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA3 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA4 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA5 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA6 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA7 compute buffer size =   546.02 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    22.02 MiB
llama_init_from_model: graph nodes  = 931
llama_init_from_model: graph splits = 9
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 32 (n_threads_batch = 32) / 64 | CUDA : ARCHS = 520,610,700,750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | AMX_INT8 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1222.31 ms
perplexity: calculating perplexity over 569 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 0.99 seconds per pass - ETA 2.33 minutes
[1]4.0789,[2]5.2737,[3]5.1919,[4]5.4710,[5]5.6287,[6]6.1699,[7]6.4971,[8]6.9027,[9]7.4698,[10]7.7724,[11]7.8250,[12]8.0378,[13]8.5570,[14]8.1026,[15]7.9195,[16]7.7333,[17]7.3773,[18]7.5402,[19]7.2137,[20]7.1564,[21]7.0219,[22]6.9744,[23]6.7293,[24]6.5157,[25]6.3967,[26]6.1797,[27]6.0963,[28]6.0740,[29]5.9989,[30]6.0658,[31]6.0728,[32]6.1006,[33]6.1076,[34]6.1570,[35]6.1576,[36]6.2338,[37]6.2973,[38]6.2321,[39]6.3180,[40]6.2704,[41]6.2392,[42]6.2704,[43]6.2759,[44]6.2372,[45]6.2164,[46]6.2862,[47]6.3454,[48]6.3628,[49]6.4009,[50]6.4549,[51]6.4807,[52]6.4987,[53]6.5322,[54]6.5204,[55]6.5916,[56]6.5853,[57]6.6172,[58]6.6558,[59]6.6792,[60]6.7246,[61]6.7368,[62]6.7849,[63]6.8487,[64]6.9054,[65]6.9536,[66]7.0088,[67]7.0002,[68]7.0082,[69]6.9964,[70]7.0213,[71]7.0512,[72]7.0738,[73]7.0761,[74]7.0392,[75]7.0221,[76]7.0342,[77]7.0287,[78]6.9792,[79]6.9806,[80]6.9736,[81]7.0205,[82]7.0303,[83]7.0132,[84]7.0364,[85]7.0636,[86]7.0668,[87]7.0877,[88]7.0599,[89]7.0735,[90]7.0752,[91]7.0563,[92]7.0828,[93]7.0956,[94]7.1185,[95]7.1166,[96]7.1662,[97]7.1696,[98]7.1831,[99]7.2499,[100]7.2764,[101]7.2810,[102]7.2753,[103]7.2697,[104]7.2584,[105]7.2789,[106]7.3133,[107]7.3537,[108]7.3965,[109]7.4800,[110]7.4856,[111]7.4755,[112]7.5252,[113]7.5656,[114]7.5472,[115]7.5475,[116]7.5538,[117]7.5337,[118]7.5381,[119]7.5349,[120]7.5174,[121]7.5092,[122]7.4875,[123]7.4765,[124]7.4487,[125]7.4445,[126]7.4031,[127]7.3748,[128]7.3427,[129]7.3210,[130]7.3134,[131]7.3125,[132]7.3189,[133]7.3160,[134]7.3069,[135]7.2833,[136]7.2954,[137]7.2973,[138]7.3292,[139]7.3181,[140]7.2975,[141]7.3143,[142]7.2836,[143]7.2552,[144]7.2161,[145]7.1758,[146]7.1236,[147]7.0660,[148]7.0338,[149]7.0019,[150]6.9756,[151]6.9508,[152]6.9312,[153]6.8953,[154]6.8640,[155]6.8349,[156]6.8008,[157]6.7771,[158]6.7632,[159]6.7396,[160]6.7261,[161]6.7196,[162]6.7108,[163]6.7005,[164]6.7093,[165]6.7012,[166]6.7241,[167]6.7464,[168]6.7858,[169]6.8245,[170]6.8403,[171]6.8823,[172]6.9131,[173]6.9617,[174]7.0073,[175]7.0120,[176]7.0073,[177]7.0392,[178]7.0550,[179]7.0450,[180]7.0495,[181]7.0490,[182]7.0651,[183]7.0749,[184]7.0738,[185]7.0915,[186]7.1082,[187]7.1195,[188]7.1176,[189]7.1295,[190]7.1557,[191]7.1810,[192]7.1895,[193]7.1978,[194]7.1896,[195]7.1815,[196]7.1651,[197]7.1468,[198]7.1683,[199]7.1885,[200]7.2048,[201]7.1874,[202]7.1948,[203]7.1759,[204]7.1593,[205]7.1433,[206]7.1321,[207]7.1328,[208]7.1329,[209]7.1172,[210]7.1084,[211]7.0925,[212]7.0879,[213]7.0744,[214]7.0555,[215]7.0302,[216]7.0169,[217]7.0142,[218]7.0014,[219]6.9968,[220]6.9747,[221]6.9577,[222]6.9502,[223]6.9443,[224]6.9268,[225]6.9133,[226]6.8971,[227]6.8870,[228]6.8862,[229]6.8767,[230]6.8626,[231]6.8712,[232]6.8640,[233]6.8543,[234]6.8633,[235]6.8700,[236]6.8762,[237]6.8741,[238]6.8915,[239]6.8928,[240]6.9057,[241]6.9157,[242]6.9299,[243]6.9420,[244]6.9500,[245]6.9578,[246]6.9640,[247]6.9747,[248]6.9980,[249]7.0202,[250]7.0273,[251]7.0360,[252]7.0277,[253]7.0054,[254]6.9878,[255]6.9681,[256]6.9631,[257]6.9597,[258]6.9629,[259]6.9616,[260]6.9579,[261]6.9486,[262]6.9365,[263]6.9309,[264]6.9192,[265]6.9109,[266]6.8928,[267]6.8938,[268]6.8835,[269]6.8793,[270]6.8735,[271]6.8703,[272]6.8623,[273]6.8623,[274]6.8456,[275]6.8301,[276]6.8008,[277]6.8090,[278]6.8182,[279]6.8148,[280]6.8159,[281]6.8159,[282]6.8187,[283]6.8253,[284]6.8437,[285]6.8471,[286]6.8522,[287]6.8612,[288]6.8716,[289]6.8842,[290]6.8781,[291]6.8651,[292]6.8624,[293]6.8609,[294]6.8567,[295]6.8503,[296]6.8545,[297]6.8599,[298]6.8583,[299]6.8562,[300]6.8550,[301]6.8490,[302]6.8529,[303]6.8557,[304]6.8508,[305]6.8464,[306]6.8493,[307]6.8388,[308]6.8392,[309]6.8491,[310]6.8541,[311]6.8533,[312]6.8621,[313]6.8568,[314]6.8511,[315]6.8647,[316]6.8735,[317]6.8887,[318]6.8939,[319]6.9032,[320]6.8961,[321]6.8952,[322]6.8915,[323]6.8851,[324]6.8877,[325]6.8842,[326]6.8805,[327]6.8881,[328]6.8845,[329]6.8994,[330]6.8986,[331]6.8974,[332]6.8877,[333]6.8799,[334]6.8793,[335]6.8775,[336]6.8749,[337]6.8686,[338]6.8553,[339]6.8491,[340]6.8401,[341]6.8320,[342]6.8286,[343]6.8274,[344]6.8220,[345]6.8085,[346]6.8181,[347]6.8227,[348]6.8221,[349]6.8125,[350]6.8151,[351]6.8179,[352]6.8244,[353]6.8208,[354]6.8123,[355]6.8210,[356]6.8325,[357]6.8539,[358]6.8727,[359]6.8904,[360]6.9036,[361]6.9220,[362]6.9360,[363]6.9503,[364]6.9554,[365]6.9591,[366]6.9687,[367]6.9681,[368]6.9768,[369]6.9888,[370]7.0022,[371]7.0110,[372]7.0155,[373]7.0227,[374]7.0323,[375]7.0444,[376]7.0473,[377]7.0393,[378]7.0282,[379]7.0281,[380]7.0415,[381]7.0529,[382]7.0544,[383]7.0591,[384]7.0510,[385]7.0479,[386]7.0529,[387]7.0570,[388]7.0582,[389]7.0644,[390]7.0664,[391]7.0649,[392]7.0666,[393]7.0608,[394]7.0551,[395]7.0526,[396]7.0466,[397]7.0511,[398]7.0545,[399]7.0579,[400]7.0595,[401]7.0717,[402]7.0742,[403]7.0796,[404]7.0769,[405]7.0727,[406]7.0763,[407]7.0839,[408]7.0943,[409]7.0930,[410]7.0906,[411]7.0957,[412]7.1039,[413]7.1012,[414]7.0997,[415]7.0908,[416]7.0911,[417]7.1078,[418]7.1118,[419]7.1241,[420]7.1249,[421]7.1250,[422]7.1251,[423]7.1189,[424]7.1184,[425]7.1099,[426]7.1087,[427]7.1132,[428]7.1066,[429]7.1085,[430]7.0985,[431]7.0950,[432]7.0918,[433]7.0919,[434]7.0836,[435]7.0767,[436]7.0701,[437]7.0690,[438]7.0592,[439]7.0599,[440]7.0601,[441]7.0498,[442]7.0413,[443]7.0386,[444]7.0389,[445]7.0365,[446]7.0404,[447]7.0471,[448]7.0388,[449]7.0372,[450]7.0385,[451]7.0436,[452]7.0479,[453]7.0492,[454]7.0544,[455]7.0549,[456]7.0628,[457]7.0685,[458]7.0682,[459]7.0665,[460]7.0614,[461]7.0532,[462]7.0488,[463]7.0564,[464]7.0558,[465]7.0508,[466]7.0490,[467]7.0411,[468]7.0448,[469]7.0563,[470]7.0601,[471]7.0610,[472]7.0641,[473]7.0613,[474]7.0595,[475]7.0581,[476]7.0522,[477]7.0465,[478]7.0453,[479]7.0391,[480]7.0341,[481]7.0320,[482]7.0292,[483]7.0218,[484]7.0190,[485]7.0262,[486]7.0263,[487]7.0226,[488]7.0222,[489]7.0223,[490]7.0176,[491]7.0261,[492]7.0226,[493]7.0216,[494]7.0195,[495]7.0190,[496]7.0210,[497]7.0216,[498]7.0184,[499]7.0225,[500]7.0143,[501]7.0116,[502]7.0074,[503]7.0061,[504]7.0020,[505]6.9922,[506]6.9887,[507]6.9902,[508]6.9885,[509]6.9851,[510]6.9790,[511]6.9794,[512]6.9833,[513]6.9895,[514]6.9857,[515]6.9836,[516]6.9796,[517]6.9821,[518]6.9784,[519]6.9825,[520]6.9800,[521]6.9855,[522]6.9886,[523]6.9862,[524]6.9943,[525]7.0040,[526]7.0032,[527]7.0133,[528]7.0143,[529]7.0080,[530]7.0079,[531]7.0127,[532]7.0151,[533]7.0138,[534]7.0052,[535]6.9950,[536]6.9991,[537]6.9922,[538]6.9863,[539]6.9762,[540]6.9644,[541]6.9620,[542]6.9664,[543]6.9722,[544]6.9723,[545]6.9805,[546]6.9822,[547]6.9849,[548]6.9907,[549]6.9988,[550]7.0031,[551]7.0117,[552]7.0103,[553]7.0085,[554]7.0059,[555]7.0063,[556]7.0080,[557]7.0095,[558]7.0168,[559]7.0204,[560]7.0241,[561]7.0261,[562]7.0277,[563]7.0234,[564]7.0252,[565]7.0287,[566]7.0351,[567]7.0369,[568]7.0462,[569]7.0365,
Final estimate: PPL = 7.0365 +/- 0.04455

llama_perf_context_print:        load time =    2465.36 ms
llama_perf_context_print: prompt eval time =   40159.12 ms / 291328 tokens (    0.14 ms per token,  7254.34 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =   64212.84 ms / 291329 tokens
