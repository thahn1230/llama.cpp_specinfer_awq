ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 6: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 7: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
build: 4479 (a4f3f5d8) with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu
llama_model_load_from_file: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA1 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA2 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA3 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA4 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA5 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA6 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA7 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 164 tensors from mymodel/gemma-2b-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 2b
llama_model_loader: - kv   3:                       general.organization str              = Google
llama_model_loader: - kv   4:                           general.basename str              = gemma
llama_model_loader: - kv   5:                         general.size_label str              = 2B
llama_model_loader: - kv   6:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   7:                     gemma.embedding_length u32              = 2048
llama_model_loader: - kv   8:                          gemma.block_count u32              = 18
llama_model_loader: - kv   9:                  gemma.feed_forward_length u32              = 16384
llama_model_loader: - kv  10:                 gemma.attention.head_count u32              = 8
llama_model_loader: - kv  11:              gemma.attention.head_count_kv u32              = 1
llama_model_loader: - kv  12:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  14:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                tokenizer.ggml.eot_token_id u32              = 107
llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - kv  29:                          general.file_type u32              = 7
llama_model_loader: - type  f32:   37 tensors
llama_model_loader: - type q8_0:  127 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 2.48 GiB (8.50 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 217
load: token to piece cache size = 1.6014 MB
print_info: arch             = gemma
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 2048
print_info: n_layer          = 18
print_info: n_head           = 8
print_info: n_head_kv        = 1
print_info: n_rot            = 256
print_info: n_swa            = 0
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 8
print_info: n_embd_k_gqa     = 256
print_info: n_embd_v_gqa     = 256
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 16384
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 2B
print_info: model params     = 2.51 B
print_info: general.name     = Gemma 2b
print_info: vocab type       = SPM
print_info: n_vocab          = 256000
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 107 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 227 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 107 '<end_of_turn>'
print_info: max token length = 48
load_tensors: offloading 18 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 19/19 layers to GPU
load_tensors:        CUDA0 model buffer size =   334.73 MiB
load_tensors:        CUDA1 model buffer size =   223.16 MiB
load_tensors:        CUDA2 model buffer size =   334.73 MiB
load_tensors:        CUDA3 model buffer size =   223.16 MiB
load_tensors:        CUDA4 model buffer size =   223.16 MiB
load_tensors:        CUDA5 model buffer size =   334.73 MiB
load_tensors:        CUDA6 model buffer size =   223.16 MiB
load_tensors:        CUDA7 model buffer size =   642.84 MiB
load_tensors:   CPU_Mapped model buffer size =   531.25 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 512
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 18, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =     6.00 MiB
llama_kv_cache_init:      CUDA1 KV buffer size =     4.00 MiB
llama_kv_cache_init:      CUDA2 KV buffer size =     6.00 MiB
llama_kv_cache_init:      CUDA3 KV buffer size =     4.00 MiB
llama_kv_cache_init:      CUDA4 KV buffer size =     4.00 MiB
llama_kv_cache_init:      CUDA5 KV buffer size =     6.00 MiB
llama_kv_cache_init:      CUDA6 KV buffer size =     4.00 MiB
llama_kv_cache_init:      CUDA7 KV buffer size =     2.00 MiB
llama_init_from_model: KV self size  =   36.00 MiB, K (f16):   18.00 MiB, V (f16):   18.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     3.91 MiB
llama_init_from_model: pipeline parallelism enabled (n_copies=4)
llama_init_from_model:      CUDA0 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA1 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA2 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA3 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA4 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA5 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA6 compute buffer size =   104.01 MiB
llama_init_from_model:      CUDA7 compute buffer size =   536.02 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    20.02 MiB
llama_init_from_model: graph nodes  = 601
llama_init_from_model: graph splits = 9
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 32 (n_threads_batch = 32) / 64 | CUDA : ARCHS = 520,610,700,750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | AMX_INT8 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1220.17 ms
perplexity: calculating perplexity over 569 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 0.82 seconds per pass - ETA 1.93 minutes
[1]5.6360,[2]7.0040,[3]6.8908,[4]7.2450,[5]7.4066,[6]8.1775,[7]8.5786,[8]9.1743,[9]10.0142,[10]10.4029,[11]10.5156,[12]10.8843,[13]11.7026,[14]10.9716,[15]10.6289,[16]10.3224,[17]9.7763,[18]10.0644,[19]9.6010,[20]9.4630,[21]9.2789,[22]9.2163,[23]8.8539,[24]8.5630,[25]8.4111,[26]8.1463,[27]8.0365,[28]8.0000,[29]7.8878,[30]7.9857,[31]7.9762,[32]8.0177,[33]8.0809,[34]8.1441,[35]8.1285,[36]8.1977,[37]8.2870,[38]8.1748,[39]8.2899,[40]8.2118,[41]8.1437,[42]8.1647,[43]8.1898,[44]8.1295,[45]8.0929,[46]8.1684,[47]8.2406,[48]8.2434,[49]8.2980,[50]8.3629,[51]8.3894,[52]8.4241,[53]8.4587,[54]8.4439,[55]8.5252,[56]8.5017,[57]8.5254,[58]8.5851,[59]8.6283,[60]8.6943,[61]8.7091,[62]8.7616,[63]8.8558,[64]8.9444,[65]9.0118,[66]9.0865,[67]9.0677,[68]9.0837,[69]9.0716,[70]9.1007,[71]9.1442,[72]9.1815,[73]9.1838,[74]9.1320,[75]9.1039,[76]9.1225,[77]9.1332,[78]9.0877,[79]9.0810,[80]9.0795,[81]9.1254,[82]9.1492,[83]9.1133,[84]9.1598,[85]9.2032,[86]9.2158,[87]9.2370,[88]9.1979,[89]9.2146,[90]9.2225,[91]9.1947,[92]9.2316,[93]9.2405,[94]9.2679,[95]9.2704,[96]9.3449,[97]9.3442,[98]9.3569,[99]9.4407,[100]9.4814,[101]9.4942,[102]9.4800,[103]9.4955,[104]9.4765,[105]9.5006,[106]9.5524,[107]9.6014,[108]9.6619,[109]9.7625,[110]9.7946,[111]9.7740,[112]9.8394,[113]9.8874,[114]9.8548,[115]9.8656,[116]9.8740,[117]9.8564,[118]9.8664,[119]9.8557,[120]9.8315,[121]9.8146,[122]9.7998,[123]9.7896,[124]9.7676,[125]9.7594,[126]9.7211,[127]9.6874,[128]9.6506,[129]9.6198,[130]9.6062,[131]9.6053,[132]9.6085,[133]9.5940,[134]9.5833,[135]9.5494,[136]9.5660,[137]9.5712,[138]9.6115,[139]9.5921,[140]9.5745,[141]9.5916,[142]9.5451,[143]9.5035,[144]9.4469,[145]9.3896,[146]9.3114,[147]9.2287,[148]9.1900,[149]9.1451,[150]9.1083,[151]9.0721,[152]9.0409,[153]8.9928,[154]8.9465,[155]8.9059,[156]8.8562,[157]8.8227,[158]8.8051,[159]8.7742,[160]8.7570,[161]8.7488,[162]8.7463,[163]8.7310,[164]8.7399,[165]8.7278,[166]8.7650,[167]8.7926,[168]8.8403,[169]8.8879,[170]8.9105,[171]8.9615,[172]8.9963,[173]9.0588,[174]9.1191,[175]9.1344,[176]9.1334,[177]9.1898,[178]9.2129,[179]9.2338,[180]9.2423,[181]9.2419,[182]9.2658,[183]9.2780,[184]9.2725,[185]9.2958,[186]9.3174,[187]9.3307,[188]9.3233,[189]9.3406,[190]9.3747,[191]9.4022,[192]9.4062,[193]9.4149,[194]9.4009,[195]9.3857,[196]9.3650,[197]9.3421,[198]9.3695,[199]9.4005,[200]9.4215,[201]9.3961,[202]9.4012,[203]9.3799,[204]9.3602,[205]9.3345,[206]9.3224,[207]9.3226,[208]9.3159,[209]9.2956,[210]9.2783,[211]9.2612,[212]9.2514,[213]9.2326,[214]9.2080,[215]9.1768,[216]9.1591,[217]9.1530,[218]9.1393,[219]9.1331,[220]9.1051,[221]9.0833,[222]9.0726,[223]9.0647,[224]9.0411,[225]9.0251,[226]9.0015,[227]8.9861,[228]8.9837,[229]8.9703,[230]8.9597,[231]8.9658,[232]8.9521,[233]8.9382,[234]8.9496,[235]8.9524,[236]8.9582,[237]8.9588,[238]8.9819,[239]8.9832,[240]9.0004,[241]9.0177,[242]9.0345,[243]9.0520,[244]9.0607,[245]9.0705,[246]9.0811,[247]9.0937,[248]9.1212,[249]9.1468,[250]9.1546,[251]9.1670,[252]9.1583,[253]9.1315,[254]9.1115,[255]9.0865,[256]9.0807,[257]9.0788,[258]9.0844,[259]9.0846,[260]9.0833,[261]9.0713,[262]9.0590,[263]9.0514,[264]9.0353,[265]9.0259,[266]9.0017,[267]9.0022,[268]8.9887,[269]8.9781,[270]8.9689,[271]8.9639,[272]8.9539,[273]8.9512,[274]8.9299,[275]8.9133,[276]8.8719,[277]8.8855,[278]8.9000,[279]8.8926,[280]8.8966,[281]8.8992,[282]8.9027,[283]8.9134,[284]8.9357,[285]8.9378,[286]8.9451,[287]8.9590,[288]8.9722,[289]8.9916,[290]8.9825,[291]8.9655,[292]8.9591,[293]8.9529,[294]8.9492,[295]8.9421,[296]8.9466,[297]8.9525,[298]8.9494,[299]8.9486,[300]8.9426,[301]8.9364,[302]8.9435,[303]8.9473,[304]8.9395,[305]8.9367,[306]8.9409,[307]8.9256,[308]8.9291,[309]8.9418,[310]8.9443,[311]8.9450,[312]8.9598,[313]8.9517,[314]8.9412,[315]8.9606,[316]8.9759,[317]8.9947,[318]8.9997,[319]9.0093,[320]8.9997,[321]8.9985,[322]8.9925,[323]8.9824,[324]8.9875,[325]8.9816,[326]8.9764,[327]8.9891,[328]8.9799,[329]8.9990,[330]8.9976,[331]8.9926,[332]8.9796,[333]8.9682,[334]8.9658,[335]8.9644,[336]8.9636,[337]8.9566,[338]8.9402,[339]8.9350,[340]8.9217,[341]8.9101,[342]8.9027,[343]8.8992,[344]8.8904,[345]8.8725,[346]8.8838,[347]8.8878,[348]8.8867,[349]8.8744,[350]8.8773,[351]8.8826,[352]8.8882,[353]8.8867,[354]8.8780,[355]8.8888,[356]8.9015,[357]8.9274,[358]8.9542,[359]8.9827,[360]9.0007,[361]9.0226,[362]9.0414,[363]9.0602,[364]9.0666,[365]9.0700,[366]9.0831,[367]9.0813,[368]9.0930,[369]9.1099,[370]9.1284,[371]9.1397,[372]9.1468,[373]9.1577,[374]9.1696,[375]9.1857,[376]9.1879,[377]9.1744,[378]9.1570,[379]9.1551,[380]9.1706,[381]9.1864,[382]9.1875,[383]9.1950,[384]9.1843,[385]9.1804,[386]9.1854,[387]9.1909,[388]9.1951,[389]9.2018,[390]9.2061,[391]9.2071,[392]9.2104,[393]9.2014,[394]9.1957,[395]9.1922,[396]9.1855,[397]9.1910,[398]9.1956,[399]9.2000,[400]9.2032,[401]9.2188,[402]9.2234,[403]9.2332,[404]9.2301,[405]9.2261,[406]9.2329,[407]9.2421,[408]9.2568,[409]9.2549,[410]9.2512,[411]9.2583,[412]9.2677,[413]9.2678,[414]9.2648,[415]9.2510,[416]9.2493,[417]9.2751,[418]9.2785,[419]9.2980,[420]9.2975,[421]9.2990,[422]9.2979,[423]9.2873,[424]9.2882,[425]9.2748,[426]9.2734,[427]9.2808,[428]9.2705,[429]9.2699,[430]9.2559,[431]9.2539,[432]9.2495,[433]9.2485,[434]9.2362,[435]9.2260,[436]9.2174,[437]9.2134,[438]9.1998,[439]9.1999,[440]9.1996,[441]9.1891,[442]9.1783,[443]9.1736,[444]9.1717,[445]9.1678,[446]9.1722,[447]9.1819,[448]9.1721,[449]9.1714,[450]9.1731,[451]9.1805,[452]9.1884,[453]9.1912,[454]9.1993,[455]9.2025,[456]9.2140,[457]9.2231,[458]9.2253,[459]9.2210,[460]9.2145,[461]9.2027,[462]9.1965,[463]9.2050,[464]9.2029,[465]9.1925,[466]9.1894,[467]9.1836,[468]9.1884,[469]9.2023,[470]9.2111,[471]9.2118,[472]9.2140,[473]9.2084,[474]9.2028,[475]9.1984,[476]9.1910,[477]9.1823,[478]9.1802,[479]9.1718,[480]9.1651,[481]9.1614,[482]9.1579,[483]9.1467,[484]9.1441,[485]9.1529,[486]9.1521,[487]9.1473,[488]9.1466,[489]9.1454,[490]9.1395,[491]9.1512,[492]9.1487,[493]9.1461,[494]9.1427,[495]9.1420,[496]9.1448,[497]9.1446,[498]9.1390,[499]9.1439,[500]9.1350,[501]9.1328,[502]9.1279,[503]9.1266,[504]9.1222,[505]9.1078,[506]9.1029,[507]9.1031,[508]9.0995,[509]9.0944,[510]9.0868,[511]9.0868,[512]9.0912,[513]9.0991,[514]9.0911,[515]9.0891,[516]9.0845,[517]9.0879,[518]9.0810,[519]9.0857,[520]9.0825,[521]9.0887,[522]9.0942,[523]9.0908,[524]9.1065,[525]9.1176,[526]9.1174,[527]9.1309,[528]9.1334,[529]9.1242,[530]9.1251,[531]9.1325,[532]9.1354,[533]9.1316,[534]9.1202,[535]9.1050,[536]9.1107,[537]9.1007,[538]9.0925,[539]9.0792,[540]9.0621,[541]9.0600,[542]9.0675,[543]9.0738,[544]9.0741,[545]9.0840,[546]9.0859,[547]9.0882,[548]9.0955,[549]9.1065,[550]9.1099,[551]9.1223,[552]9.1221,[553]9.1196,[554]9.1158,[555]9.1164,[556]9.1195,[557]9.1224,[558]9.1330,[559]9.1375,[560]9.1411,[561]9.1426,[562]9.1458,[563]9.1395,[564]9.1431,[565]9.1480,[566]9.1576,[567]9.1608,[568]9.1750,[569]9.1620,
Final estimate: PPL = 9.1620 +/- 0.06213

llama_perf_context_print:        load time =    1478.38 ms
llama_perf_context_print: prompt eval time =   33386.52 ms / 291328 tokens (    0.11 ms per token,  8725.92 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =   57626.49 ms / 291329 tokens
