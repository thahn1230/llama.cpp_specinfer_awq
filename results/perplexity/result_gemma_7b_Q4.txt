ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 6: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 7: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
build: 4479 (a4f3f5d8) with cc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0 for x86_64-linux-gnu
llama_model_load_from_file: using device CUDA0 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA1 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA2 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA3 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA4 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA5 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA6 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_load_from_file: using device CUDA7 (NVIDIA GeForce RTX 4090) - 23826 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 254 tensors from mymodel/gemma-7b-Q4_K.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gemma
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gemma 7b
llama_model_loader: - kv   3:                       general.organization str              = Google
llama_model_loader: - kv   4:                           general.basename str              = gemma
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                       gemma.context_length u32              = 8192
llama_model_loader: - kv   7:                     gemma.embedding_length u32              = 3072
llama_model_loader: - kv   8:                          gemma.block_count u32              = 28
llama_model_loader: - kv   9:                  gemma.feed_forward_length u32              = 24576
llama_model_loader: - kv  10:                 gemma.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gemma.attention.head_count_kv u32              = 16
llama_model_loader: - kv  12:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  13:                 gemma.attention.key_length u32              = 256
llama_model_loader: - kv  14:               gemma.attention.value_length u32              = 256
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = default
llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,256000]  = ["<pad>", "<eos>", "<bos>", "<unk>", ...
llama_model_loader: - kv  18:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 2
llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 1
llama_model_loader: - kv  22:            tokenizer.ggml.unknown_token_id u32              = 3
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                tokenizer.ggml.eot_token_id u32              = 107
llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - kv  29:                          general.file_type u32              = 15
llama_model_loader: - type  f32:   57 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.96 GiB (4.99 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: special tokens cache size = 217
load: token to piece cache size = 1.6014 MB
print_info: arch             = gemma
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 256
print_info: n_swa            = 0
print_info: n_embd_head_k    = 256
print_info: n_embd_head_v    = 256
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 4096
print_info: n_embd_v_gqa     = 4096
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 24576
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 7B
print_info: model params     = 8.54 B
print_info: general.name     = Gemma 7b
print_info: vocab type       = SPM
print_info: n_vocab          = 256000
print_info: n_merges         = 0
print_info: BOS token        = 2 '<bos>'
print_info: EOS token        = 1 '<eos>'
print_info: EOT token        = 107 '<end_of_turn>'
print_info: UNK token        = 3 '<unk>'
print_info: PAD token        = 0 '<pad>'
print_info: LF token         = 227 '<0x0A>'
print_info: EOG token        = 1 '<eos>'
print_info: EOG token        = 107 '<end_of_turn>'
print_info: max token length = 48
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:        CUDA0 model buffer size =   659.06 MiB
load_tensors:        CUDA1 model buffer size =   615.75 MiB
load_tensors:        CUDA2 model buffer size =   467.23 MiB
load_tensors:        CUDA3 model buffer size =   637.41 MiB
load_tensors:        CUDA4 model buffer size =   615.75 MiB
load_tensors:        CUDA5 model buffer size =   467.23 MiB
load_tensors:        CUDA6 model buffer size =   659.06 MiB
load_tensors:        CUDA7 model buffer size =   955.61 MiB
load_tensors:   CPU_Mapped model buffer size =   615.23 MiB
llama_init_from_model: n_seq_max     = 4
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 512
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache_init:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache_init:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache_init:      CUDA7 KV buffer size =    64.00 MiB
llama_init_from_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_init_from_model:  CUDA_Host  output buffer size =     3.91 MiB
llama_init_from_model: pipeline parallelism enabled (n_copies=4)
llama_init_from_model:      CUDA0 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA1 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA2 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA3 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA4 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA5 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA6 compute buffer size =   148.01 MiB
llama_init_from_model:      CUDA7 compute buffer size =   546.02 MiB
llama_init_from_model:  CUDA_Host compute buffer size =    22.02 MiB
llama_init_from_model: graph nodes  = 931
llama_init_from_model: graph splits = 9
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 32 (n_threads_batch = 32) / 64 | CUDA : ARCHS = 520,610,700,750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | AMX_INT8 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1216.08 ms
perplexity: calculating perplexity over 569 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 0.92 seconds per pass - ETA 2.17 minutes
[1]4.5400,[2]5.7042,[3]5.5794,[4]5.8732,[5]6.0382,[6]6.6105,[7]6.9836,[8]7.4063,[9]8.0334,[10]8.3197,[11]8.3871,[12]8.6305,[13]9.2060,[14]8.7199,[15]8.5085,[16]8.3459,[17]7.9453,[18]8.1403,[19]7.7857,[20]7.7134,[21]7.5685,[22]7.5191,[23]7.2383,[24]7.0243,[25]6.9093,[26]6.6854,[27]6.5871,[28]6.5593,[29]6.4728,[30]6.5403,[31]6.5490,[32]6.5706,[33]6.5706,[34]6.6304,[35]6.6281,[36]6.7060,[37]6.7715,[38]6.6971,[39]6.7878,[40]6.7340,[41]6.6990,[42]6.7275,[43]6.7314,[44]6.6857,[45]6.6646,[46]6.7337,[47]6.7946,[48]6.8094,[49]6.8508,[50]6.9112,[51]6.9371,[52]6.9642,[53]7.0014,[54]6.9861,[55]7.0643,[56]7.0564,[57]7.0909,[58]7.1293,[59]7.1569,[60]7.2076,[61]7.2237,[62]7.2760,[63]7.3457,[64]7.4090,[65]7.4640,[66]7.5219,[67]7.5086,[68]7.5136,[69]7.5044,[70]7.5320,[71]7.5642,[72]7.5927,[73]7.5908,[74]7.5442,[75]7.5223,[76]7.5353,[77]7.5286,[78]7.4746,[79]7.4756,[80]7.4678,[81]7.5149,[82]7.5270,[83]7.5066,[84]7.5275,[85]7.5548,[86]7.5556,[87]7.5759,[88]7.5444,[89]7.5599,[90]7.5620,[91]7.5418,[92]7.5710,[93]7.5853,[94]7.6112,[95]7.6087,[96]7.6659,[97]7.6703,[98]7.6861,[99]7.7540,[100]7.7835,[101]7.7868,[102]7.7792,[103]7.7796,[104]7.7670,[105]7.7910,[106]7.8321,[107]7.8767,[108]7.9229,[109]8.0147,[110]8.0235,[111]8.0122,[112]8.0675,[113]8.1115,[114]8.0947,[115]8.0977,[116]8.1103,[117]8.0899,[118]8.0945,[119]8.0924,[120]8.0734,[121]8.0674,[122]8.0489,[123]8.0408,[124]8.0142,[125]8.0150,[126]7.9710,[127]7.9440,[128]7.9113,[129]7.8858,[130]7.8766,[131]7.8738,[132]7.8788,[133]7.8799,[134]7.8683,[135]7.8434,[136]7.8579,[137]7.8598,[138]7.8949,[139]7.8828,[140]7.8603,[141]7.8758,[142]7.8410,[143]7.8086,[144]7.7645,[145]7.7212,[146]7.6627,[147]7.5989,[148]7.5629,[149]7.5265,[150]7.4960,[151]7.4672,[152]7.4454,[153]7.4065,[154]7.3709,[155]7.3404,[156]7.3020,[157]7.2763,[158]7.2616,[159]7.2353,[160]7.2212,[161]7.2145,[162]7.2062,[163]7.1956,[164]7.2074,[165]7.1980,[166]7.2246,[167]7.2493,[168]7.2926,[169]7.3330,[170]7.3515,[171]7.3986,[172]7.4342,[173]7.4893,[174]7.5382,[175]7.5451,[176]7.5391,[177]7.5756,[178]7.5936,[179]7.5819,[180]7.5873,[181]7.5870,[182]7.6061,[183]7.6175,[184]7.6180,[185]7.6368,[186]7.6540,[187]7.6665,[188]7.6659,[189]7.6768,[190]7.7054,[191]7.7336,[192]7.7414,[193]7.7532,[194]7.7442,[195]7.7355,[196]7.7186,[197]7.6991,[198]7.7210,[199]7.7464,[200]7.7655,[201]7.7463,[202]7.7551,[203]7.7354,[204]7.7174,[205]7.6990,[206]7.6878,[207]7.6881,[208]7.6879,[209]7.6702,[210]7.6597,[211]7.6409,[212]7.6351,[213]7.6222,[214]7.6007,[215]7.5728,[216]7.5582,[217]7.5559,[218]7.5417,[219]7.5376,[220]7.5128,[221]7.4929,[222]7.4848,[223]7.4804,[224]7.4620,[225]7.4474,[226]7.4313,[227]7.4209,[228]7.4209,[229]7.4115,[230]7.3979,[231]7.4073,[232]7.3990,[233]7.3878,[234]7.3984,[235]7.4055,[236]7.4118,[237]7.4093,[238]7.4286,[239]7.4297,[240]7.4446,[241]7.4568,[242]7.4734,[243]7.4878,[244]7.4977,[245]7.5073,[246]7.5148,[247]7.5268,[248]7.5510,[249]7.5773,[250]7.5855,[251]7.5973,[252]7.5906,[253]7.5655,[254]7.5458,[255]7.5223,[256]7.5168,[257]7.5118,[258]7.5141,[259]7.5125,[260]7.5075,[261]7.4977,[262]7.4841,[263]7.4774,[264]7.4643,[265]7.4545,[266]7.4336,[267]7.4344,[268]7.4214,[269]7.4155,[270]7.4082,[271]7.4061,[272]7.3980,[273]7.3965,[274]7.3793,[275]7.3626,[276]7.3298,[277]7.3379,[278]7.3485,[279]7.3440,[280]7.3440,[281]7.3459,[282]7.3487,[283]7.3546,[284]7.3741,[285]7.3783,[286]7.3846,[287]7.3930,[288]7.4038,[289]7.4169,[290]7.4090,[291]7.3951,[292]7.3918,[293]7.3899,[294]7.3860,[295]7.3776,[296]7.3814,[297]7.3871,[298]7.3861,[299]7.3831,[300]7.3813,[301]7.3752,[302]7.3805,[303]7.3825,[304]7.3768,[305]7.3717,[306]7.3757,[307]7.3647,[308]7.3648,[309]7.3752,[310]7.3804,[311]7.3799,[312]7.3919,[313]7.3858,[314]7.3791,[315]7.3923,[316]7.4022,[317]7.4190,[318]7.4229,[319]7.4324,[320]7.4244,[321]7.4232,[322]7.4181,[323]7.4101,[324]7.4136,[325]7.4096,[326]7.4059,[327]7.4137,[328]7.4087,[329]7.4248,[330]7.4239,[331]7.4228,[332]7.4128,[333]7.4039,[334]7.4024,[335]7.4009,[336]7.3972,[337]7.3904,[338]7.3758,[339]7.3698,[340]7.3598,[341]7.3511,[342]7.3460,[343]7.3441,[344]7.3382,[345]7.3228,[346]7.3336,[347]7.3384,[348]7.3374,[349]7.3265,[350]7.3288,[351]7.3308,[352]7.3379,[353]7.3336,[354]7.3243,[355]7.3336,[356]7.3460,[357]7.3687,[358]7.3899,[359]7.4086,[360]7.4235,[361]7.4427,[362]7.4583,[363]7.4732,[364]7.4792,[365]7.4835,[366]7.4938,[367]7.4929,[368]7.5035,[369]7.5170,[370]7.5321,[371]7.5412,[372]7.5463,[373]7.5539,[374]7.5645,[375]7.5782,[376]7.5813,[377]7.5722,[378]7.5600,[379]7.5586,[380]7.5724,[381]7.5845,[382]7.5852,[383]7.5898,[384]7.5813,[385]7.5785,[386]7.5842,[387]7.5885,[388]7.5895,[389]7.5968,[390]7.5986,[391]7.5974,[392]7.5993,[393]7.5920,[394]7.5857,[395]7.5834,[396]7.5770,[397]7.5825,[398]7.5877,[399]7.5918,[400]7.5928,[401]7.6065,[402]7.6096,[403]7.6166,[404]7.6145,[405]7.6106,[406]7.6143,[407]7.6223,[408]7.6336,[409]7.6320,[410]7.6300,[411]7.6354,[412]7.6443,[413]7.6423,[414]7.6402,[415]7.6300,[416]7.6306,[417]7.6503,[418]7.6550,[419]7.6695,[420]7.6697,[421]7.6693,[422]7.6704,[423]7.6633,[424]7.6624,[425]7.6528,[426]7.6521,[427]7.6576,[428]7.6499,[429]7.6523,[430]7.6410,[431]7.6371,[432]7.6336,[433]7.6338,[434]7.6242,[435]7.6163,[436]7.6096,[437]7.6082,[438]7.5973,[439]7.5986,[440]7.5987,[441]7.5881,[442]7.5800,[443]7.5777,[444]7.5784,[445]7.5754,[446]7.5796,[447]7.5863,[448]7.5773,[449]7.5752,[450]7.5763,[451]7.5822,[452]7.5862,[453]7.5874,[454]7.5936,[455]7.5938,[456]7.6026,[457]7.6086,[458]7.6086,[459]7.6068,[460]7.6021,[461]7.5927,[462]7.5880,[463]7.5962,[464]7.5960,[465]7.5909,[466]7.5887,[467]7.5803,[468]7.5851,[469]7.5978,[470]7.6022,[471]7.6033,[472]7.6057,[473]7.6030,[474]7.6008,[475]7.5987,[476]7.5920,[477]7.5857,[478]7.5845,[479]7.5777,[480]7.5721,[481]7.5696,[482]7.5660,[483]7.5575,[484]7.5539,[485]7.5613,[486]7.5610,[487]7.5573,[488]7.5571,[489]7.5577,[490]7.5525,[491]7.5617,[492]7.5581,[493]7.5569,[494]7.5557,[495]7.5540,[496]7.5561,[497]7.5563,[498]7.5535,[499]7.5583,[500]7.5490,[501]7.5470,[502]7.5420,[503]7.5410,[504]7.5369,[505]7.5261,[506]7.5222,[507]7.5236,[508]7.5217,[509]7.5184,[510]7.5118,[511]7.5119,[512]7.5157,[513]7.5226,[514]7.5177,[515]7.5153,[516]7.5113,[517]7.5136,[518]7.5093,[519]7.5140,[520]7.5111,[521]7.5173,[522]7.5207,[523]7.5186,[524]7.5280,[525]7.5390,[526]7.5383,[527]7.5497,[528]7.5515,[529]7.5445,[530]7.5444,[531]7.5497,[532]7.5526,[533]7.5515,[534]7.5423,[535]7.5309,[536]7.5361,[537]7.5288,[538]7.5227,[539]7.5115,[540]7.4984,[541]7.4962,[542]7.5014,[543]7.5076,[544]7.5077,[545]7.5161,[546]7.5176,[547]7.5206,[548]7.5270,[549]7.5368,[550]7.5413,[551]7.5498,[552]7.5481,[553]7.5473,[554]7.5447,[555]7.5444,[556]7.5462,[557]7.5480,[558]7.5560,[559]7.5598,[560]7.5631,[561]7.5657,[562]7.5667,[563]7.5624,[564]7.5647,[565]7.5686,[566]7.5758,[567]7.5784,[568]7.5886,[569]7.5780,
Final estimate: PPL = 7.5780 +/- 0.04914

llama_perf_context_print:        load time =    1889.24 ms
llama_perf_context_print: prompt eval time =   42817.53 ms / 291328 tokens (    0.15 ms per token,  6803.94 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time =   68473.07 ms / 291329 tokens
